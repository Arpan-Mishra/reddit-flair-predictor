{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Collecting subreddit \"India\" Data\n",
    "\n",
    "Source: https://towardsdatascience.com/scraping-reddit-data-1c0af3040768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'praw'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-43dc093a6ce7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[1;31m# data manipulation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpraw\u001b[0m \u001b[1;31m# python reddit API wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'praw'"
     ]
    }
   ],
   "source": [
    "import pandas as pd # data manipulation \n",
    "import praw # python reddit API wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate ourselves by creating a Reddit instance and providing client credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credentials generated from the reddit developers applications page\n",
    "my_client_id = '8KS6G6Nt9BU9sg'\n",
    "my_client_secret = 'CkeVFda-vf0DbseDb0eEr1YMpJo'\n",
    "user = 'reddit_scrape'\n",
    "\n",
    "reddit = praw.Reddit(client_id=my_client_id, client_secret=my_client_secret, user_agent=user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get subreddit data \n",
    "Using the `reddit` instance from the previous section, we can aquire top 1000 posts or 1000 hottest posts or latest posts from reddit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_posts = 1000   # Number of posts we want in our data\n",
    "new_posts = reddit.subreddit('India').top(limit=num_of_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the titles of the top_posts in this subreddit\n",
    "for post in new_posts:\n",
    "    print(post.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data type of the `new_posts` to identify the class it belongs to and to get more information about the various methods and attributes. \n",
    "Answer - object of the ListingGenerator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(new_posts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure subreddit data into a pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of features that have to be saved in the dataset. These are data columns that we will extract from our dataset. \n",
    "\n",
    "**Feature Description:**\n",
    "\n",
    "* **Title:** The title of the submission.\n",
    "* **Score:** The number of upvotes for the submission.\n",
    "* **ID:** ID of the submission.\n",
    "* **Subreddit:** Provides an instance of Subreddit.\n",
    "* **URL:** The URL the submission links to, or the permalink if a selfpost.\n",
    "* **Orginal**: Whether or not the submission has been set as original content.\n",
    "* **num_comments:** The number of comments on the submission.\n",
    "* **Body:** The submissionsâ€™ selftext - an empty string if a link post.\n",
    "* **created_on:** Time the submission was created, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Title', 'Score', 'ID', 'Subreddit', 'URL', 'Original', 'num_comments', 'Flair', 'Body', 'created_on']\n",
    "posts = [] # List containing the data from individual reddit posts. Each item will be a new entry in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "india_sub = reddit.subreddit('India')\n",
    "\n",
    "# Loop through each subreddit entry and append that t\n",
    "for post in india_sub.top(limit=num_of_posts):\n",
    "    posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.is_original_content, post.num_comments, post.link_flair_text, post.selftext, post.created])\n",
    "\n",
    "# Convert to a data frame \n",
    "posts = pd.DataFrame(posts, columns=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the data frame and analyse the data a little more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct the time format using the datetime library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the to_datetime function of pandas\n",
    "posts['creation_date'] = pd.to_datetime(posts['created_on'], dayfirst=True, unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop created_on column now \n",
    "posts.drop(['created_on'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after changing the `num_of_posts`, I still getting only 988 posts and a very skewed dataset. Hence, I need to create a better scraping mechanism to get a more balanced Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection: Improvised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing I will do is to create lesser number of targets for classification for more robust models and increase the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "posts['Flair'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I sorted the flairs in the descending order and picked up the most popular flairs to avoid skewed data. I will collect top posts and their comments along with some other information relavnt for the analysis and the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant flairs\n",
    "flairs = [\"AskIndia\", \"Non-Political\", \"[R]eddiquette\", \n",
    "          \"Photography\", \"Science/Technology\",\n",
    "          \"Politics\", \"Business/Finance\", \"Policy/Economy\",\n",
    "          \"Sports\", \"Food\", \"AMA\", \"Coronavirus\", \"CAA-NRC-NPR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data features that we will be collecting \n",
    "features = [\n",
    "    'Title', \n",
    "    'Score', \n",
    "    'ID',\n",
    "    'URL', \n",
    "    'num_comments', \n",
    "    'created_on', \n",
    "    'Body', \n",
    "    'Original',\n",
    "    'Flair', \n",
    "    'Comments'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subreddit instance \n",
    "subreddit = reddit.subreddit('india')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = []\n",
    "# Top 250 posts of each type \n",
    "for flair in flairs: \n",
    "    relevant_subs = subreddit.search(f\"flair_name:{flair}\", limit=250)\n",
    "    \n",
    "    for sub in relevant_subs:\n",
    "        post = []\n",
    "        post = [\n",
    "            str(sub.title),\n",
    "            str(sub.score),\n",
    "            str(sub.id),\n",
    "            str(sub.url),\n",
    "            sub.num_comments,\n",
    "            str(sub.created),\n",
    "            str(sub.selftext),\n",
    "            sub.is_original_content,\n",
    "            str(sub.link_flair_text),\n",
    "        ]\n",
    "        \n",
    "        sub.comments.replace_more(limit=0)\n",
    "        comment = ''\n",
    "        for top_comment in sub.comments:\n",
    "            comment = str(top_comment.body) + ' '        \n",
    "        \n",
    "        post.append(str(comment))# Add to the end of the list \n",
    "        posts.append(post)    # Add to the main list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a data frame \n",
    "posts = pd.DataFrame(posts, columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More detailed Data\n",
    "posts.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
